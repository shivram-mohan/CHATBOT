{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6337c0b",
   "metadata": {},
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f761084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c99486f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.5.27)\n",
      "Requirement already satisfied: dataclasses_json in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (0.5.7)\n",
      "Requirement already satisfied: langchain==0.0.142 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (0.0.142)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (1.24.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (8.2.2)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (0.27.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (1.4.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from llama-index) (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (4.0.2)\n",
      "Requirement already satisfied: gptcache>=0.1.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (0.1.21)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from langchain==0.0.142->llama-index) (2.29.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses_json->llama-index) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses_json->llama-index) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dataclasses_json->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai>=0.26.4->llama-index) (4.64.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->llama-index) (2021.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tiktoken->llama-index) (2022.4.24)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.142->llama-index) (1.3.1)\n",
      "Requirement already satisfied: cachetools in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gptcache>=0.1.7->langchain==0.0.142->llama-index) (5.3.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic<2,>=1->langchain==0.0.142->llama-index) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain==0.0.142->llama-index) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain==0.0.142->llama-index) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2->langchain==0.0.142->llama-index) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from SQLAlchemy<2,>=1->langchain==0.0.142->llama-index) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses_json->llama-index) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->openai>=0.26.4->llama-index) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index) (3.0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041eae8",
   "metadata": {},
   "source": [
    "# Basic LlamaIndex Usage Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6395b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381c47e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1321 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18731b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1448 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think Facebook's LLaMa is a great step forward in democratizing access to large language models and advancing research in this subfield of AI. It is encouraging to see that they are making the model available at several sizes and providing a model card to detail how it was built in accordance with responsible AI practices. I am also glad to see that they are releasing the model under a noncommercial license to ensure integrity and prevent misuse.\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = index.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4e5c",
   "metadata": {},
   "source": [
    "# Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2518123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb07416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cyclone_Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "376312ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 7883 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "539d38a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3910 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cyclone Freddy is a very intense tropical cyclone that traversed the southern Indian Ocean for more than five weeks in February and March 2023. It is both the longest-lasting and highest-ACE-producing tropical cyclone ever recorded worldwide. Additionally, it is the third-deadliest tropical cyclone recorded in the Southern Hemisphere, only behind 2019's Cyclone Idai and the 1973 Flores cyclone. It caused catastrophic flooding, wind damage, and loss of life in Madagascar, Mauritius, Mozambique, and Malawi. In Malawi, the rains worsened immensely, while in Mauritius, strong winds and waves were observed along the northern coast of the island, with winds in Port Louis reaching 104 km/h (65 mph) and a peak gust of 154 km/h (96 mph) observed on Signal Mountain. Flooding and gale-force winds also affected the country, resulting in one fatality and at least 500 displaced families in a variety of shelters across Mauritius. Additionally, the Taiwanese-flagged fishing trawler LV Lien Sheng Fa with a crew of 16 went missing just outside the territorial waters of Mauritius, and was later found capsized about 400 km (250 mi) northeast of Mauritius within the nation\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What is cyclone freddy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2acc",
   "metadata": {},
   "source": [
    "# Customer Support Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8bdca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19f396a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 12584 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader('./asos').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4babd878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1317 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the United Arab Emirates, you have the option of signing up for ASOS Premier, which gives you free Standard and Express delivery all year round when you spend over 150 AED. It costs 200 AED and is valid on the order you purchase it on.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77e646",
   "metadata": {},
   "source": [
    "# YouTube Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9073c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89160742",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=ILY3Q5AxPbc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0995d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1979 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "194e88fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2057 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "According to Hinduism, the universe was created through the actions of Lord Vishnu, who slept and let a lotus bloom from his navel. Lord Brahma is the creator, and Lord Shiva is the destroyer.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Who created the universe?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a05da",
   "metadata": {},
   "source": [
    "# Chatbot Class - Just include your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65a0e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0d3da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1321 tokens\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a2211bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is Llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1428 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. It is designed to be versatile and can be applied to many different use cases. It is trained on a large set of unlabeled data, which makes it ideal for fine-tuning for a variety of tasks.\n",
      "You: who created it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1366 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "Meta created LLaMA (Large Language Model Meta AI).\n",
      "You: When was it created?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1363 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "LLaMA was created in 2021.\n"
     ]
    }
   ],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "question_count = 0  # Initialize question counter\n",
    "\n",
    "while question_count < 3:  # Change the desired number of questions here\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")\n",
    "    question_count += 1  # Increment question counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b20be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
